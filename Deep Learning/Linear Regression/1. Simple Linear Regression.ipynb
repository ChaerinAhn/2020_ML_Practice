{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build hypothesis and cost\n",
    "\n",
    "## H(x)= Wx + b \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1, 2, 3, 4, 5]\n",
    "y_data = [1, 2, 3, 4, 5]\n",
    "W = tf.Variable(2.9)\n",
    "b = tf.Variable(0.5) \n",
    "#초기값은 임의의 값\n",
    "hypothesis= W * x_data + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent \n",
    "* 경사를 하강하면서 최소값을 찾음 \n",
    "* with tf.GradientTape() as tape: 이후에 나오는 것들을 tape에 기록\n",
    "* A.assign_sub(B) 뜻?\n",
    "    A = A - B\n",
    "    A -= B\n",
    "* 임의의 매우작은 값 learning rate 만큼 반영해서 업데이트해나감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0|    2.4520|     0.376| 45.660004\n",
      "   10|    1.1036|  0.003398|  0.206336\n",
      "   20|    1.0128|  -0.02091|  0.001026\n",
      "   30|    1.0065|  -0.02184|  0.000093\n",
      "   40|    1.0059|  -0.02123|  0.000083\n",
      "   50|    1.0057|  -0.02053|  0.000077\n",
      "   60|    1.0055|  -0.01984|  0.000072\n",
      "   70|    1.0053|  -0.01918|  0.000067\n",
      "   80|    1.0051|  -0.01854|  0.000063\n",
      "   90|    1.0050|  -0.01793|  0.000059\n",
      "  100|    1.0048|  -0.01733|  0.000055\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "for i in range(100+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis= W * x_data + b    \n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "#경사도 구하기 (각 변수 W, b에 대해 cost 값을 미분한 기울기 값)\n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b]) \n",
    "\n",
    "#W, b 값을 업데이트 \n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    if i % 10 == 0:\n",
    "        print(\"{:5}|{:10.4f}|{:10.4}|{:10.6f}\".format(i, W.numpy(), b.numpy(), cost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(5.00667, shape=(), dtype=float32)\n",
      "tf.Tensor(2.4946702, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 값을 대입해서 예측해보기\n",
    "print(W * 5 + b)\n",
    "print(W * 2.5 + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to minimize cost\n",
    "* 간략화된 모델로 우선 진행 H(x) = Wx   \n",
    "* for feed_W in np.linspace(-3, 5, num=15):\n",
    " -3~5까지 15개 구간으로 나눈 값을 feed_W가 가짐 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.000 |   74.66667\n",
      "-2.429 |   54.85714\n",
      "-1.857 |   38.09524\n",
      "-1.286 |   24.38095\n",
      "-0.714 |   13.71429\n",
      "-0.143 |    6.09524\n",
      " 0.429 |    1.52381\n",
      " 1.000 |    0.00000\n",
      " 1.571 |    1.52381\n",
      " 2.143 |    6.09524\n",
      " 2.714 |   13.71429\n",
      " 3.286 |   24.38095\n",
      " 3.857 |   38.09524\n",
      " 4.429 |   54.85714\n",
      " 5.000 |   74.66667\n"
     ]
    }
   ],
   "source": [
    "# pure Python 으로 Cost function 만들기\n",
    "X = np.array([1,2,3])\n",
    "Y = np.array([1,2,3])\n",
    "\n",
    "def cost_func(W, X, Y):\n",
    "    c = 0\n",
    "    for i in range(len(X)):\n",
    "        c += (W * X[i] - Y[i]) ** 2\n",
    "    return c / len(X)\n",
    "\n",
    "for feed_W in np.linspace(-3, 5, num=15):\n",
    "    curr_cost = cost_func(feed_W, X, Y)\n",
    "    print(\"{:6.3f} | {:10.5f}\".format(feed_W, curr_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.000 |   74.66667\n",
      "-2.429 |   54.85714\n",
      "-1.857 |   38.09524\n",
      "-1.286 |   24.38095\n",
      "-0.714 |   13.71429\n",
      "-0.143 |    6.09524\n",
      " 0.429 |    1.52381\n",
      " 1.000 |    0.00000\n",
      " 1.571 |    1.52381\n",
      " 2.143 |    6.09524\n",
      " 2.714 |   13.71429\n",
      " 3.286 |   24.38095\n",
      " 3.857 |   38.09524\n",
      " 4.429 |   54.85714\n",
      " 5.000 |   74.66667\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow로 Cost Function 만들기\n",
    "def cost_func2(W, X, Y):\n",
    "    hypothesis2 = X * W\n",
    "    return tf.reduce_mean(tf.square(hypothesis2 - Y))\n",
    "\n",
    "W_values = np.linspace(-3, 5, num=15)\n",
    "cost_values = []\n",
    "\n",
    "for feed_W in W_values:\n",
    "    curr_cost = cost_func(feed_W, X, Y)\n",
    "    cost_values.append(curr_cost)\n",
    "    print(\"{:6.3f} | {:10.5f}\".format(feed_W, curr_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 | 18829.7812 |  47.348293\n",
      "   10 |  3959.8613 |  22.254509\n",
      "   20 |   832.7499 |  10.746943\n",
      "   30 |   175.1255 |   5.469776\n",
      "   40 |    36.8285 |   3.049760\n",
      "   50 |     7.7449 |   1.939984\n",
      "   60 |     1.6287 |   1.431060\n",
      "   70 |     0.3425 |   1.197676\n",
      "   80 |     0.0720 |   1.090651\n",
      "   90 |     0.0151 |   1.041571\n",
      "  100 |     0.0032 |   1.019064\n",
      "  110 |     0.0007 |   1.008742\n",
      "  120 |     0.0001 |   1.004009\n",
      "  130 |     0.0000 |   1.001839\n",
      "  140 |     0.0000 |   1.000843\n",
      "  150 |     0.0000 |   1.000387\n",
      "  160 |     0.0000 |   1.000178\n",
      "  170 |     0.0000 |   1.000081\n",
      "  180 |     0.0000 |   1.000037\n",
      "  190 |     0.0000 |   1.000017\n",
      "  200 |     0.0000 |   1.000008\n",
      "  210 |     0.0000 |   1.000004\n",
      "  220 |     0.0000 |   1.000002\n",
      "  230 |     0.0000 |   1.000001\n",
      "  240 |     0.0000 |   1.000001\n",
      "  250 |     0.0000 |   1.000001\n",
      "  260 |     0.0000 |   1.000001\n",
      "  270 |     0.0000 |   1.000001\n",
      "  280 |     0.0000 |   1.000001\n",
      "  290 |     0.0000 |   1.000001\n"
     ]
    }
   ],
   "source": [
    "# gradient 함수 쓰지 않고 직접 만들어서 구해보기\n",
    "\n",
    "tf.random.set_seed(0) # 다음에 다시 돌렸을때도 똑같이 나올 수 있도록 고정\n",
    "X = [1, 2, 3, 4]\n",
    "Y = [1, 2, 3, 4]\n",
    "W= tf.Variable(tf.random.normal([1], -100, 100)) #normal 분포 중 임의의 값 1개\n",
    "\n",
    "\n",
    "for step in range(300):\n",
    "    hypothesis3 = W * X\n",
    "    cost3 = tf.reduce_mean(tf.square(hypothesis3 - Y))\n",
    "    alpha = 0.01\n",
    "    gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X))\n",
    "    descent = W - tf.multiply(alpha, gradient)\n",
    "    W.assign(descent)\n",
    "    \n",
    "    if step % 10 ==0:\n",
    "        print('{:5} | {:10.4f} | {:10.6f}'.format(step, cost3.numpy(), W.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
